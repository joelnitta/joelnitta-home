{
  "hash": "df19b517a5f721119a7dc975ea83dca2",
  "result": {
    "markdown": "---\ntitle: \"Managing bioinformatics pipelines with R\"\ndescription:\n  How to combine Conda, Docker, and R to run modular, reproducible bioinformatics pipelines\ndate: \"2021-11-16\"\ndate-modified: today\nimage: \"img/t-k-9AxFJaNySB8-unsplash.jpg\"\ncitation:\n  url: https://www.joelnitta.com/posts/2021-11-16_r-bioinfo-flow/\ncategories:\n  - workflow\n  - reproducibility\n---\n\n\n<!--------------- setup post ----------------->\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n## tl;dr\n\n- The `targets` R package is great for managing bioinformatics workflows\n- `renv`, Conda, and Docker can be combined so that all steps are modular and reproducible\n- Demo available at [https://github.com/joelnitta/targets_bioinfo_example](https://github.com/joelnitta/targets_bioinfo_example)\n\n![Image by [T K](https://unsplash.com/@realaxer) on [unsplash](https://unsplash.com/photos/9AxFJaNySB8).](img/t-k-9AxFJaNySB8-unsplash.jpg){#fig-image-1}\n\nBioinformatics projects tend to have a similar pattern: they all start with raw data, then pass the data through various programs until arriving at the final result. These \"pipelines\" can become very long and complicated, so there are many platforms that automate this process either relying on code (e.g., [nextflow](https://www.nextflow.io/), [CWL](https://www.commonwl.org/)) or graphical interfaces (e.g., [galaxy](https://galaxyproject.org/)). Python's [snakemake](https://snakemake.readthedocs.io/en/stable/) is also commonly used for this purpose. That got me thinking---can we do this in R?\n\n## What I'm looking for in a pipeline manager\n\nThese are some qualities that I want to see in a pipeline manager.\n\n1. **Automated**: I should be able to run one central script that will orchestrate the whole pipeline, rather than manually keeping track of which step depends on which and when each needs to be run. \n2. **Efficient**: The pipeline manager should keep track of what is out of date and only re-run those parts, rather than re-run the whole pipeline each time.\n3. **Reproducible**: Software packages should be isolated and version controlled so that the same input results in the same output on any machine.\n\n## Enter `targets`\n\nThe [targets](https://docs.ropensci.org/targets/) R package pretty much fits the bill perfectly for Points **1** and **2**. `targets` completely automates the workflow, so that the user doesn't have to manually run steps, and guarantees that the output is up-to-date (if the workflow is designed correctly). Furthermore, it has capabilities for easily looping and running processes in parallel, so it scales quite well to large analyses. I won't go into too many details of how to use `targets` here, since it has an excellent [user manual](https://books.ropensci.org/targets/).\n\n## `targets` meets Docker\n\nHowever, `targets` by itself isn't quite enough to meet all of my bioinformatics needs. What about Point **3**---how can we make `targets` workflows reproducible?\n\nMost bioinformatics tools are open-source software packages that have a command-line interface (CLI). Furthermore, these days, most well-established bioinformatics tools have Docker images[^1] available to run them. Good sources to find Docker images for bioinformatics software are [Bioconda](http://bioconda.github.io/) or [Biocontainers](https://biocontainers.pro/)[^2]. Docker frees us from manual installations and [dependency hell](https://en.wikipedia.org/wiki/Dependency_hell), as well as vastly improving reproducibility, since all the software versions are fixed within the container. \n\nSo **I will run most of the steps of the pipeline in available Docker containers**.\n\n## Avoiding Docker-in-Docker\n\n![Image by [Giordano Rossoni](https://unsplash.com/@reddgio) on [unsplash](https://unsplash.com/photos/czu8X_gfpP0).](img/giordano-rossoni-czu8X_gfpP0-unsplash.jpg){#fig-image-2}\n\nHowever, I then encounter a problem: what about the environment to run R, `targets`, and launch the Docker containers? That environment should be version-controlled and reproducible too. Normally my solution to create such an environment **is** Docker, but it's generally a bad idea to try and run [docker from within docker](https://jpetazzo.github.io/2015/09/03/do-not-use-docker-in-docker-for-ci/)[^3]\n\nThe solution I reached is to use two more environment managers: [Conda](https://docs.conda.io/en/latest/)[^4] and [renv](https://rstudio.github.io/renv/articles/renv.html). I use Conda for **running R**, and `renv` for **managing R packages**.\n\n## First things first: Set up the project\n\nI need to explain one thing before continuing: for this example, I'm following [the practice of using a \"project\" for the analysis](http://swcarpentry.github.io/good-enough-practices-in-scientific-computing/). This simply means all of the files needed for the analysis are put in a single folder (with subfolders as necessary), and that folder is used as the \"home base\" for the project. So if I type some command at the command line prompt, it is assumed that the project folder is the [current working directory](https://en.wikipedia.org/wiki/Working_directory). The two main tools I use to maintain the pipeline, `renv` and `targets`, both [rely on this concept](https://books.ropensci.org/targets/walkthrough.html#file-structure).\n\nFrom the command line, that just looks like:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nmkdir targets_bioinfo_example\ncd targets_bioinfo_example\n```\n:::\n\n\nNext, let's download some files that I will use in the subsequent steps (don't worry about what these do yet; I will explain each one below):\n\n<!-- for some reason include=FALSE doesn't seem to work with bash, so write two chunks-->\n\n::: {.cell}\n\n```{.bash .cell-code}\n# environment.yml\ncurl https://raw.githubusercontent.com/joelnitta/targets_bioinfo_example/main/environment.yml > environment.yml\n# renv.lock\ncurl https://raw.githubusercontent.com/joelnitta/targets_bioinfo_example/main/renv.lock > renv.lock\n# _targets.R\ncurl https://raw.githubusercontent.com/joelnitta/joelnitta-home/main/posts/2021-11-16_r-bioinfo-flow/_targets.R > _targets.R\n```\n:::\n\n\n\n\nFrom here on, I assume we are running everything a folder called `targets_bioinfo_example` containing the files `environment.yml`, `renv.lock`, and `_targets.R`.\n\nAlso, although I've mentioned several pieces of software so far, there are only two required for this workflow: [Conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html) and [Docker](https://docs.docker.com/get-docker/). Make sure those are both installed before continuing.\n\n## Running R with Conda\n\nConda environments [can be specified using a `yml` file](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#creating-an-environment-from-an-environment-yml-file), often named `environment.yml`.\n\n[This is the `environment.yml` file](https://raw.githubusercontent.com/joelnitta/targets_bioinfo_example/main/environment.yml) for this project:\n\n```\nname: bioinfo-example-env\nchannels:\n  - conda-forge\n  - bioconda\n  - defaults\ndependencies:\n  - r-renv=0.14.*\n```\n\nIt's quite short: all it does is install `renv` and its dependencies (which includes R). Here I've specified the most recent major version[^5] of `renv`, which will come with R v4.1.1.\n\nWe can recreate the Conda environment from `environment.yml` (you should have downloaded it [above](#first-things-first-set-up-the-project)) with:\n\n<!-- Chunk to show, but don't run-->\n\n::: {.cell}\n\n```{.bash .cell-code}\nconda env create -f environment.yml\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\nCollecting package metadata (repodata.json): ...working... done\nSolving environment: ...working... done\n\n\n==> WARNING: A newer version of conda exists. <==\n  current version: 23.5.0\n  latest version: 23.7.4\n\nPlease update conda by running\n\n    $ conda update -n base -c defaults conda\n\nOr to minimize the number of packages updated during conda update use\n\n     conda install conda=23.7.4\n\n\n\nDownloading and Extracting Packages\n\nPreparing transaction: ...working... done\nVerifying transaction: ...working... done\nExecuting transaction: ...working... done\n#\n# To activate this environment, use\n#\n#     $ conda activate bioinfo-example-env\n#\n# To deactivate an active environment, use\n#\n#     $ conda deactivate\n```\n:::\n:::\n\n\nAs the output says near the bottom, run `conda activate bioinfo-example-env` to enter this environment, then from there you can use R as usual with `R`.\n\nOn my computer, this looks like:\n\n```\n(base) Joels-iMac:targets_bioinfo_example joelnitta$ conda activate bioinfo-example-env\n(bioinfo-example-env) Joels-iMac:targets_bioinfo_example joelnitta$ R\n\nR version 4.1.1 (2021-08-10) -- \"Kick Things\"\nCopyright (C) 2021 The R Foundation for Statistical Computing\nPlatform: x86_64-apple-darwin13.4.0 (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n  Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\n> \n```\n\nNotice the change from `(base)` to `(bioinfo-example-env)`, indicating that we are now inside the Conda environment.\n\nNow we have a fixed version of R, with a fixed version of `renv`.\n\n## Maintain R packages with `renv`\n\nThe next step is to use `renv` to install and track R package versions. `renv` does this with a [\"lock file\"](https://rstudio.github.io/renv/articles/lockfile.html), which is essentially a specification of every package needed to run the code, its version, and where it comes from. \n\nThis is what the entry in [the `renv.lock` file for this project](https://raw.githubusercontent.com/joelnitta/targets_bioinfo_example/main/renv.lock) for the package `Matrix` looks like:\n\n```\n\"Matrix\": {\n  \"Package\": \"Matrix\",\n  \"Version\": \"1.5-4\",\n  \"Source\": \"Repository\",\n  \"Repository\": \"CRAN\",\n  \"Hash\": \"e779c7d9f35cc364438578f334cffee2\",\n  \"Requirements\": [\n    \"lattice\"\n  ]\n}\n```\n\nAssuming `renv.lock` is present in the working directory (you should have downloaded it [above](#first-things-first-set-up-the-project)), we can install all packages needed for this example by running the following in R within the Conda environment:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrenv::activate() # Turn on renv\nrenv::restore() # Install packages\n```\n:::\n\n\nYou should see something like this[^6]:\n\n```\nThe following package(s) will be updated:\n\n# CRAN ===============================\n- Matrix          [* -> 1.3-4]\n- R6              [* -> 2.5.1]\n- Rcpp            [* -> 1.0.7]\n- RcppArmadillo   [* -> 0.10.6.0.0]\n- RcppParallel    [* -> 5.1.4]\n- assertthat      [* -> 0.2.1]\n- babelwhale      [* -> 1.0.3]\n- callr           [* -> 3.7.0]\n\n...\n```\n\nIf you look at the contents of the project directory, you will also notice a new folder called `renv` that contains all of the R packages we just installed.\n\n## Putting it all together\n\nOK, now we can run R and Docker from a reproducible environment. What is the best way to run Docker from R? There are some functions in base R for running external commands ([`system()`](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/system), [`system2()`](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/system2)) as well as the excellent [processx](https://github.com/r-lib/processx) package. Here, though I will use the [babelwhale](https://github.com/dynverse/babelwhale) package, which provides some nice wrappers to run Docker (or [Singularity](https://sylabs.io/singularity/))[^7].\n\n[Here is an example `_targets.R` file](https://raw.githubusercontent.com/joelnitta/joelnitta-home/main/_posts/2021-11-16_r-bioinfo-flow/_targets.R) using `babelwhale` to run Docker. This workflow downloads a pair of fasta files, then trims low-quality bases using the [fastp](https://github.com/OpenGene/fastp) program[^8]:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(targets)\nlibrary(tarchetypes)\nlibrary(babelwhale)\n\n# Set babelwhale backend for running containers\n# (here, we are using Docker, not Singularity)\nset_default_config(create_docker_config())\n\n# Define workflow\nlist(\n\t# Download example fastq files\n\ttar_file(\n\t\tread_1, { \n\t\t\tdownload.file(\n\t\t\t\turl = \"https://raw.githubusercontent.com/OpenGene/fastp/master/testdata/R1.fq\",\n\t\t\t\tdestfile = \"R1.fq\")\n\t\t\t\"R1.fq\"\n\t\t}\n\t),\n\ttar_file(\n\t\tread_2, { \n\t\t\tdownload.file(\n\t\t\t\turl = \"https://raw.githubusercontent.com/OpenGene/fastp/master/testdata/R2.fq\",\n\t\t\t\tdestfile = \"R2.fq\")\n\t\t\t\"R2.fq\"\n\t\t}\n\t),\n\t# Clean the fastq file with fastp\n\ttar_file(\n\t\tfastp_out, {\n\t\t\tbabelwhale::run(\n\t\t\t\t# Name of docker image, with tag specifying version\n\t\t\t\t\"quay.io/biocontainers/fastp:0.23.1--h79da9fb_0\",\n\t\t\t\t# Command to run\n\t\t\t\tcommand = \"fastp\",\n\t\t\t\t# Arguments to the command\n\t\t\t\targs = c(\n\t\t\t\t\t# fastq input files\n\t\t\t\t\t\"-i\", paste0(\"/wd/\", read_1), \n\t\t\t\t\t\"-I\", paste0(\"/wd/\", read_2), \n\t\t\t\t\t# fastq output files\n\t\t\t\t\t\"-o\", \"/wd/R1_trim.fq\",\n\t\t\t\t  \"-O\", \"/wd/R2_trim.fq\",\n\t\t\t\t\t# trim report file\n\t\t\t\t\t\"-h\", \"/wd/trim_report.html\"),\n\t\t\t\t# Volume mounting specification\n\t\t\t\t# this uses getwd(), but here::here() is also a good method\n\t\t\t\tvolumes = paste0(getwd(), \":/wd/\")\n\t\t\t)\n\t\t\tc(\"R1_trim.fq\", \"R2_trim.fq\", \"trim_report.html\")\n\t\t}\n\t)\n)\n```\n:::\n\n\nIn order to run this `targets` workflow, the above code must be saved as `_targets.R` in the project root directory (you should have downloaded it [above](#first-things-first-set-up-the-project)).\n\nFinally, everything is in place! All we need to do now is run `targets::tar_make()`, sit back, and enjoy the show:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntargets::tar_make()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n▶ start target read_1\ntrying URL 'https://raw.githubusercontent.com/OpenGene/fastp/master/testdata/R1.fq'\nContent type 'text/plain; charset=utf-8' length 3041 bytes\n==================================================\ndownloaded 3041 bytes\n\n● built target read_1 [0.157 seconds]\n▶ start target read_2\ntrying URL 'https://raw.githubusercontent.com/OpenGene/fastp/master/testdata/R2.fq'\nContent type 'text/plain; charset=utf-8' length 3343 bytes\n==================================================\ndownloaded 3343 bytes\n\n● built target read_2 [0.161 seconds]\n▶ start target fastp_out\n● built target fastp_out [3.16 seconds]\n▶ end pipeline [3.53 seconds]\n```\n:::\n:::\n\n\nYou should be able to confirm that the read files were downloaded, cleaned, and a report generated in your working directory. Also, notice there is a new folder called `_targets`. This contains the metadata that `targets` uses to track each step of the pipeline (generally it should not be modified by hand; the same goes for the `renv` folder).\n\n## Next steps\n\n![Image by [JOHN TOWNER](https://unsplash.com/@heytowner) on [unsplash](https://unsplash.com/photos/3Kv48NS4WUU)](img/john-towner-3Kv48NS4WUU-unsplash.jpg){#fig-image-3}\n\nThe example workflow just consists of a couple of steps, but I hope you can see how they are chained together: `fastp_out` depends on `read_1` and `read_2`. We could add a third step that uses `fastp_out` for something else, and so forth.\n\nWe can also see this by visualizing the pipeline:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntargets::tar_visnetwork()\n```\n\n::: {.cell-output-display}\n```{=html}\n<div class=\"visNetwork html-widget html-fill-item-overflow-hidden html-fill-item\" id=\"htmlwidget-7cdb15ca2bfa53951167\" style=\"width:672px;height:480px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-7cdb15ca2bfa53951167\">{\"x\":{\"nodes\":{\"name\":[\"fastp_out\",\"read_1\",\"read_2\"],\"type\":[\"stem\",\"stem\",\"stem\"],\"status\":[\"uptodate\",\"uptodate\",\"uptodate\"],\"seconds\":[3.16,0.157,0.161],\"bytes\":[420567,3041,3343],\"branches\":[null,null,null],\"label\":[\"fastp_out\",\"read_1\",\"read_2\"],\"color\":[\"#354823\",\"#354823\",\"#354823\"],\"id\":[\"fastp_out\",\"read_1\",\"read_2\"],\"level\":[2,1,1],\"shape\":[\"dot\",\"dot\",\"dot\"]},\"edges\":{\"from\":[\"read_1\",\"read_2\"],\"to\":[\"fastp_out\",\"fastp_out\"],\"arrows\":[\"to\",\"to\"]},\"nodesToDataframe\":true,\"edgesToDataframe\":true,\"options\":{\"width\":\"100%\",\"height\":\"100%\",\"nodes\":{\"shape\":\"dot\",\"physics\":false},\"manipulation\":{\"enabled\":false},\"edges\":{\"smooth\":{\"type\":\"cubicBezier\",\"forceDirection\":\"horizontal\"}},\"physics\":{\"stabilization\":false},\"interaction\":{\"zoomSpeed\":1},\"layout\":{\"hierarchical\":{\"enabled\":true,\"direction\":\"LR\"}}},\"groups\":null,\"width\":null,\"height\":null,\"idselection\":{\"enabled\":false,\"style\":\"width: 150px; height: 26px\",\"useLabels\":true,\"main\":\"Select by id\"},\"byselection\":{\"enabled\":false,\"style\":\"width: 150px; height: 26px\",\"multiple\":false,\"hideColor\":\"rgba(200,200,200,0.5)\",\"highlight\":false},\"main\":{\"text\":\"\",\"style\":\"font-family:Georgia, Times New Roman, Times, serif;font-weight:bold;font-size:20px;text-align:center;\"},\"submain\":null,\"footer\":null,\"background\":\"rgba(0, 0, 0, 0)\",\"highlight\":{\"enabled\":true,\"hoverNearest\":false,\"degree\":{\"from\":1,\"to\":1},\"algorithm\":\"hierarchical\",\"hideColor\":\"rgba(200,200,200,0.5)\",\"labelOnly\":true},\"collapse\":{\"enabled\":true,\"fit\":false,\"resetHighlight\":true,\"clusterOptions\":null,\"keepCoord\":true,\"labelSuffix\":\"(cluster)\"},\"legend\":{\"width\":0.2,\"useGroups\":false,\"position\":\"right\",\"ncol\":1,\"stepX\":100,\"stepY\":100,\"zoom\":true,\"nodes\":{\"label\":[\"Up to date\",\"Stem\"],\"color\":[\"#354823\",\"#899DA4\"],\"shape\":[\"dot\",\"dot\"]},\"nodesToDataframe\":true},\"tooltipStay\":300,\"tooltipStyle\":\"position: fixed;visibility:hidden;padding: 5px;white-space: nowrap;font-family: verdana;font-size:14px;font-color:#000000;background-color: #f5f4ed;-moz-border-radius: 3px;-webkit-border-radius: 3px;border-radius: 3px;border: 1px solid #808074;box-shadow: 3px 3px 10px rgba(0, 0, 0, 0.2);\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\nTo keep things simple for this post, I have written the workflow as a single R script, but that's not really the ideal way to do it. You can see that the syntax is rather verbose, and such a script would rapidly become very long. The best practice for `targets` workflows is to write the targets plan and the functions that build each target separately, as `_targets.R` and `functions.R`, respectively.\n\nBy splitting the plan from the functions this way, our `_targets.R` file becomes much shorter and more readable:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(targets)\nlibrary(tarchetypes)\nlibrary(babelwhale)\n\n# Set babelwhale backend for running containers\nset_default_config(create_docker_config())\n\n# Load functions\nsource(\"R/functions.R\")\n\ntar_plan(\n\t# Download example fastq files\n\ttar_file(read_1, download_read(\"R1.fq\")),\n\ttar_file(read_2, download_read(\"R2.fq\")),\n\t# Clean the fastq files with fastp\n\ttar_file(\n\t\tfastp_out, \n\t\tfastp(read_1, read_2, \"R1_trim.fq\", \"R2_trim.fq\", \"trim_report.html\"\n\t\t)\n\t)\n)\n```\n:::\n\n\nYou can see how it provides a high-level overview of each step in the workflow, without getting bogged down in the details. And the best part is, you **don't have to install `fastp`** (or any other software used for a particular step)! Docker takes care of that for you.\n\nFurthermore, thanks to `targets`, if one part of the workflow changes and we run `tar_make()` again, only the part that changed will be run. Try it by deleting `R1.fq`, then run `tar_make()` again and see what happens.\n\nI have made this plan and the accompanying `functions.R` file available at this repo: https://github.com/joelnitta/targets_bioinfo_example. Please check it out!\n\n## Conclusion\n\nI am really excited about using `targets` for reproducibly managing bioinformatics workflows from R. I hope this helps others who may want to do the same!\n\n[^1]: [Docker images](https://docs.docker.com/get-started/overview/) are basically completely self-contained computing environments, such that the software inside the image is exactly the same no matter where it is run. A major benefit of using a docker image is that you don't have to install all of the various dependencies for a particular package: it all comes bundled in the image. And if the image has been tagged (versioned) correctly, you can specify the exact software version and know that the results won't change in the future.\n\n[^2]: [Bioconda](https://bioconda.github.io/) creates a Docker image for each package, which is listed in [Biocontainers](https://biocontainers.pro/) and uploaded to [Quay.io](https://quay.io/organization/biocontainers), so Bioconda and Biocontainers are largely overlapping. I find the Bioconda interface easier to use for finding images. You can also just try googling the name of the software you want to use plus \"docker\". If there is no available image, [you can build one yourself](https://carpentries-incubator.github.io/docker-introduction/05-creating-container-images/index.html), but that's outside the scope of this post.\n\n[^3]: Think *Inception*.\n\n[^4]: Conda was originally developed for managing python and python packages, but it has expanded greatly and works as a general software package manager.\n\n[^5]: The asterisk in `r-renv=0.14.*` indicates to install the most recent version with the `0.14` version number.\n\n[^6]: I'm not actually running this command and showing the output, since this post is already rendered using `renv`, and running `renv` within `renv` is also getting too *Inception*-y!\n\n[^7]: I typically use Docker, but Singularity may be a good option if you want to run your workflow on a machine where you don't have root privileges (such as on a cluster). Docker requires root privileges to install, but Singularity doesn't (for that matter neither does Conda). I have not tested any of this with Singularity.\n\n[^8]: I won't go into the details of the `targets` syntax here, but I highly recommend [this chapter in the `targets` manual](https://books.ropensci.org/targets/files.html) for working with external files, which are very common in bioinformatics workflows.\n\n## Reproducibility {.appendix}\n\n- [Source code](https://github.com/joelnitta/joelnitta-home/tree/main/posts/2021-11-16_r-bioinfo-flow/index.qmd)\n- [`renv` lockfile](https://github.com/joelnitta/joelnitta-home/tree/main/posts/2021-11-16_r-bioinfo-flow/renv.lock)",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/htmlwidgets-1.6.2/htmlwidgets.js\"></script>\n<link href=\"../../site_libs/vis-9.1.0/vis-network.min.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/vis-9.1.0/vis-network.min.js\"></script>\n<script src=\"../../site_libs/visNetwork-binding-2.1.2/visNetwork.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}