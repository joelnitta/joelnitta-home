---
title: "#fernfriday 2021"
description: |
  In which I answer the burning question on everybody's minds: what was
  the most popular fern of 2021??
author:
  - first_name: "Joel"
    last_name: "Nitta"
    url: https://joelnitta.com
    affiliation: University of Tokyo
    affiliation_url: http://iwasakilab.k.u-tokyo.ac.jp
    orcid_id: 0000-0003-4719-7472
date: 2022-01-24
preview: 'img/new-zealand-fantail-3729278_960_720.jpg'
creative_commons: CC BY
citation_url: https://joelnitta.com/friday-ferns-2021 
repository_url: https://github.com/joelnitta/joelnitta-home
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_depth: 3
params:
  slug: friday-ferns-2021
  date: 2022-01-24
  repo: joelnitta/joelnitta-home
  site: https://joelnitta.com
editor_options: 
  chunk_output_type: console
draft: false
---

<!--------------- setup post ----------------->

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE)
refinery::renv_load(paste(params$date, params$slug, sep = "_"))
library(tweetrmd)
library(webshot2)
```

```{r xaringanExtra-clipboard, echo = FALSE}
# Enable copy-paste of code chunks
# xaringanExtra::use_clipboard()
htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "<i class=\"fa fa-clipboard fa-2x\"></i>",
    success_text = "<i class=\"fa fa-check fa-2x\" style=\"color: #90BE6D\"></i>",
    error_text = "<i class=\"fa fa-times-circle fa-2x\" style=\"color: #F94144\"></i>"
  ),
  rmarkdown::html_dependency_font_awesome()
)
```

<!--------------- post ----------------->

## tl;dr

I show how to analyze #fernfriday tweets from 2021, including

- Scraping tweets with `snscraper`
- Extracting species names with `gntagger`
- Loading JSON data into R with `jsonlite`

```{r fern-bird}
#| echo = FALSE,
#| fig.cap = "Image by [jewelie108](https://pixabay.com/users/jewelie108-2884314/) on [pixabay](https://pixabay.com/photos/new-zealand-fantail-bird-3729278/)." 

knitr::include_graphics("img/new-zealand-fantail-3729278_960_720.jpg")
```

One of the saving graces of twitter is the existence of things like [#fernfriday](https://twitter.com/hashtag/fernfriday?src=hashtag_click) and other hashtags of people's favorite organisms.

Recently, [\@HannahOish](https://twitter.com/HannahOish) posted [this brief analysis](https://twitter.com/HannahOish/status/1477267832018776068) of the [#FishADay](https://twitter.com/hashtag/FishADay?src=hashtag_click) hashtag:

```{r hannah-tweet, echo = FALSE, cache = TRUE}
tweet_screenshot(tweet_url("HannahOish", "1477267832018776068"))
```

Upon seeing it, I immediately knew what I had to do next: **find out the most popular fern species of [#fernfriday](https://twitter.com/hashtag/fernfriday?src=hashtag_click) in 2021!**

First let's load some packages.

```{r load-packages, cache = FALSE}
library(tidyverse)
library(ggrepel)
library(janitor)
library(fuzzyjoin)
```

## Getting tweets with snscrape

There are at least two R packages to get twitter data in R,  [rtweet](https://github.com/ropensci/rtweet) and [tweetR](https://cran.r-project.org/web/packages/twitteR/README.html)

There is just one problem: they rely on the [official twitter API](https://developer.twitter.com/en/docs/tutorials/getting-started-with-r-and-v2-of-the-twitter-api), which is **limited to tweets in the past week to 10 days or so**, unless you want to shell out \$\$\$ for a paid API account... which I don't.

So I will get around this pesky problem by using the [snscrape](https://github.com/JustAnotherArchivist/snscrape) twitter scraper written in python ^[This is probably not within twitter's user guidelines, so use at your own discretion].

### Installation

snscrape can be installed using [pip](https://pip.pypa.io/en/stable/).

My preferred method for handling python is to use the conda environment manager. We can [use pip within conda](https://www.anaconda.com/blog/using-pip-in-a-conda-environment) to install snscrape by writing an `environment.yml` file like this (thanks to [\@hansheng0512](https://github.com/hansheng0512/tweets-scrapping-using-python)):

```
name: tweets-scraping
dependencies:
- python=3.8
- pip=21.1.3
- pip:
- git+https://github.com/JustAnotherArchivist/snscrape.git
```

Once that is in your working directory, run this to create the environment:

```
conda env create -f environment.yml
```

Now snscrape should be installed and ready to use!

### Scraping

Since `snscrape` is a command-line interface (CLI), the normal way to use it would be to type commands from the command line. But this is an R blog, so let's run it from R using the `system()` command ^[A good way to limit the number of hits when you are testing code is with the `--max-results` option, e.g., `--max-results 10`] ^[Again, inspired by [\@hansheng0512]( https://github.com/hansheng0512/tweets-scrapping-using-python/blob/master/main.py)'s code].

```{r scrape-tweets, cache = TRUE}
# Set some variables, in case we want to modify the search later
DATE_START <- "2021-01-01"
DATE_END <- "2021-12-31"
HASHTAG <- "fernfriday"
JSON_FILENAME = "ff-2021-tweets"

# Compose the command for snscrape to retrieve tweets
command <- glue::glue('snscrape --jsonl --since {DATE_START} twitter-hashtag "{HASHTAG} until:{DATE_END}" > {JSON_FILENAME}.json')

# Run the command inside the conda env
system(
  glue::glue(
    "source ~/miniconda3/etc/profile.d/conda.sh ; 
			 conda activate tweets-scraping ; 
			 {command} ; 
			 conda deactivate")
)
```

Now the tweets have been downloaded in JSON format to `ff-2021-tweets.json`.

### Loading tweets into R

We can [load the tweets into R](https://stackoverflow.com/questions/35276184/json-using-jsonlite-parsing-error-in-r) with the `jsonlite` package.

```{r load-tweets-show, echo = TRUE, eval = FALSE}
ff_tweets_raw <- jsonlite::stream_in(file("ff-2021-tweets.json"))
```

```{r load-tweets-run, echo = FALSE, eval = TRUE}
ff_tweets_raw <- jsonlite::stream_in(file(here::here("_posts/2022-01-24_friday-ferns-2021/ff-2021-tweets.json")))
```

Let's take a peek at the data.

```{r peek-data}
glimpse(ff_tweets_raw)
```

There's a lot of stuff in there! Let's clean up the data a bit and just keep the useful bits.

```{r clean-data}
ff_tweets <-
  ff_tweets_raw %>%
    as_tibble() %>%
    select(
      date, url, user, id,
      content = renderedContent,
      lang, coordinates, place,
      contains("count")
    ) %>%
  # Extract data from nested dataframes 
  mutate(
    user = select(user, username, followersCount),
    place = select(place, country),
    coordinates = select(coordinates, longitude, latitude)
    ) %>%
  unnest(c(user, place, coordinates)) %>%
  clean_names()

ff_tweets
```

That's better.

## Extracting scientific names with gntagger

So getting our twitter data wasn't that hard, now just to find all the fern species...

Uh-oh. This is easier said than done. Twitter doesn't have a JSON field for fern species!

Luckily this post *does* have some biological content. For this task I will use [`gntagger`](https://github.com/gnames/gntagger). `gntagger` is a fantastic little CLI written in GO that handles exactly this sort of situation: **it automatically detects species names in raw text** ^[OK it was probably designed more with old biodiversity literature in mind and not twitter, but it works fine either way!].

Installation [is quite simple as described in the `gntagger` docs](https://github.com/gnames/gntagger#installation), so I won't go into that any more here.

### Tagging names

`gntagger` requires a plain-text file as input, so let's write out the tweets to a plain text file with one line per tweet:

```{r gntagger-run, echo = FALSE, eval = FALSE}
ff_tweets %>%
  # Replace line breaks with spaces so we get one tweet per line
  mutate(content = str_replace_all(content, "\n", " ")) %>%
  pull(content) %>%
  write_lines(
    here::here("_posts/2022-01-24_friday-ferns-2021/ff-2021-tweet-content.txt"))
```

```{r gntagger-show, echo = TRUE, eval = TRUE}
ff_tweets %>%
  # Replace line breaks with spaces so we get one tweet per line
  mutate(content = str_replace_all(content, "\n", " ")) %>%
  pull(content) %>%
  write_lines("ff-2021-tweet-content.txt")
```

Now, open the text file with `gntagger` ^[You may need to execute this with `./gntagger` if you didn't make `gntagger` executable everywhere by putting it on your `PATH`]:

```
gntagger ff-2021-tweet-content.txt
```

You should see a screen that looks like this:

![](img/gntagger.png)

`gntagger` takes a "android" approach to tagging species names. It first uses its own algorithm to automatically identify all possible species names, then [it has a simple user interface](https://github.com/gnames/gntagger#user-interface) so the user can confirm or reject each candidate name rapidly. According to the [`gntagger` docs](https://github.com/gnames/gntagger#gntagger-), you should be able to get through about "4000 names spread over 600 pages in about 2 hours". It took me around 10--15 min. for this dataset, which included 664 candidate names to process.

It saves the intermediate output to a folder named after the input file, in this case `ff-2021-tweet-content.txt_gntagger`.

### Loading gntagger output into R

I recommend trying out `gntagger` to see how it works---it definitely looks great for parsing names from old literature!

But you don't have to go through all of the names yourself. In fact, `gntagger`'s initial "guesses" are pretty darn good, and can be used as-is for a rough analysis. Or, you can download the cleaned up version from FIXME: ADD LINK.

Let's read in the `gntagger` results, which are again JSON:

```{r load-gntagger-show, echo = TRUE, eval = FALSE}
ff_taxa_raw <- jsonlite::fromJSON("ff-2021-tweet-content.txt_gntagger/names.json")

ff_taxa <- ff_taxa_raw[["names"]] %>% 
  as_tibble() %>%
		# I'll call the name output by gntagger "taxon", since
	# not all of them are species
  select(type, annotation, taxon = name, start, end)

ff_taxa
```

```{r load-gntagger-run, echo = FALSE, eval = TRUE}
ff_taxa_raw <- jsonlite::fromJSON(here::here("_posts/2022-01-24_friday-ferns-2021/ff-2021-tweet-content.txt_gntagger/names.json"))

ff_taxa <- ff_taxa_raw[["names"]] %>% 
  as_tibble() %>%
	# I'll call the name output by gntagger "taxon", since
	# not all of them are species
  select(type, annotation, taxon = name, start, end) %>%
	# Exclude candidate names that I flagged as bad
	filter(annotation != "NotName")

ff_taxa
```

A bit about the column names we are reading in from `gntagger`:

- `type` is something automatically defined by `gntagger`. I'm not quite sure what it means... but I'm guessing "Binomial" indicates that the name is a species.
- `annotation` is the annotation I selected during the tagging process. It includes values "Accepted", "Genus", "NotName", "Species", and "Uninomial". It wasn't totally clear to me how to apply these during the tagging part. I mostly just hit the forward arrow for names that looked like species, so those are annotated as "Accepted". Towards the end I noticed I could select "Species", so a few are annotated as "Species" instead of "Accepted" ^[I actually think using a single variable for both taxonomic status ("Accepted") and rank ("Species", "Genus", etc) is rather confusing, and have filed an [issue](https://github.com/gnames/gntagger/issues/68) about this].
- `start` and `end` indicate the character position in the raw text matching the start and end of the name.

The next step is to use the `start` and `end` fields to match species names to tweets. For that, we'll need a tibble with the start and end position of each tweet.

```{r calc_start_end}
ff_tweets_start_end <-
  ff_tweets %>%
  mutate(
    # Replace line breaks with spaces so we get one tweet per line
    content = str_replace_all(content, "\n", " "),
    # Count number of characters per tweet
    num_char = nchar(content)) %>%
  select(id, num_char) %>%
  # Calculate start and end of each tweet in characters
  mutate(
    end = cumsum(num_char),
    start = end - num_char + 1,
    start = as.integer(start)) %>%
  select(id, start, end)

ff_tweets_start_end
```

Now we can use the `fuzzyjoin` package to join the species names to the tweets by position.

```{r fuzzy-join}
ff_tweets_taxa <-
ff_tweets %>%
  left_join(ff_tweets_start_end, by = "id") %>%
  # interval_left_join requires the Bioconductor IRanges package to be installed
	# and joins by columns "start" and "end" by default
  interval_left_join(
    ff_taxa
  ) %>%
  select(-matches("start|end")) %>%
  # Drop tweets without any fern names mentioned
  filter(!is.na(taxon))
```

Let's take a peek at the results.

```{r peek-post-join}
select(ff_tweets_taxa, content, taxon)
```

Yay! We've got fern names mapped to tweets (and user ID, etc). Notice that some tweets now appear duplicated, since our data are have one row per fern name mentioned, and some tweets may include multiple names.

## Data analysis (finally!)

OK, we can finally analyze the data and see **who is the most popular fern of 2021**!

Like any good data analysis, let's check the distribution of the data first.

```{r species-distribution, fig.cap = "Histogram of species occurrences in tweets"}
ff_tweets_taxa %>%
	# Filter to only taxa that look like species names
	filter(str_detect(type, "Binomial")) %>%
	count(taxon) %>%
	ggplot(aes(x = n, fill = n == 1)) + 
	geom_histogram(bins = 20) +
	labs(fill = "Singleton") +
	scale_fill_viridis_d() +
	labs(x = "n species", y = "n tweets")
```

So the vast majority of species are only tweeted about once ("singletons" in the plot), then there's a longer tail of a much smaller number of species that receive multiple tweets. Sort of like ecology---[common species are rare, and rare species are common](https://methodsblog.com/2021/04/26/fuzzyq-the-commonness-of-rarity/). Neat.

Now that we have a feel for how the data are distributed, let's see who are the rare species with multiple tweets. It's possible that some users are tweeting the same species multiple times, and that seems sort of unfair right? So we will just consider one tweet per species per user, and look at the top 10.

```{r top-species-users, fig.cap = "Top 10 species by number of users tweeting"}
ff_tweets_taxa %>% 
	# Filter to only taxa that look like species names
  filter(str_detect(type, "Binomial")) %>%
	# Only allow one "vote" per species per user
	select(taxon, username) %>% 
	unique() %>% 
	# Just look at the top 10
	# fct_lump_n() lumps the remainder into "Other"
	mutate(taxon = fct_lump_n(taxon, n = 10)) %>%
	# Exclude "Other"
	filter(taxon != "Other") %>%
	count(taxon) %>%
	mutate(taxon = fct_reorder(taxon, n)) %>%
	ggplot(aes(x = n, y = taxon)) + 
	geom_col() +
	labs(x = "n users tweeting") +
	theme(axis.title.y = element_blank())
```

Cool! Those are some solid picks. And the top 10 species here all include > 3 three unique users tweeting about them.

What about the most popular genera?

```{r top-genera-users, fig.cap = "Top 10 genera by number of users tweeting"}
ff_tweets_taxa %>% 
	# Split species names into genus and specific epithet
	separate(
		taxon, into = c("genus", "specific_epithet"), sep = " ",
		fill = "right", extra = "drop") %>%
	# Exclude names that I annotated as "Uninomial" (family names)
	filter(annotation != "Uninomial") %>%
	# Exclude abbreviated genera ("P.", etc)
	mutate(n_genus_char = nchar(genus)) %>%
	filter(n_genus_char > 3) %>%
	# Only allow one "vote" per species per user
	select(genus, username) %>% 
	unique() %>% 
	# Just look at the top 10
	# fct_lump_n() lumps the remainder into "Other"
	mutate(genus = fct_lump_n(genus, n = 10)) %>%
	# Exclude "Other"
	filter(genus != "Other") %>%
	count(genus) %>%
	mutate(genus = fct_reorder(genus, n)) %>%
	ggplot(aes(x = n, y = genus)) + 
	geom_col() +
	labs(x = "n users tweeting") +
	theme(axis.title.y = element_blank())
```

The trends change a bit: although no single *Asplenium* species gets lots of tweets, the genus as whole does. Cool!

There is obviously a lot that can be done with twitter data, and text data in general. I won't be going there. But I just want to look at one more thing...

Who are the most prolific [#fernfriday](https://twitter.com/hashtag/fernfriday?src=hashtag_click) tweeters?

```{r prolific-tweeters, fig.cap = "Number of #fernfriday tweets per user", layout="l-body-outset"}
ff_user_count <-
	ff_tweets %>%
	count(username) %>%
	mutate(username = fct_reorder(username, n))

ggplot(ff_user_count, aes(x = n, y = username)) +
	geom_col(aes(fill = n == 1)) +
	# Only label the top 10
	geom_label_repel(
		data = slice_max(ff_user_count, order_by = n, n = 10), 
		aes(label = username),
		force = 3,
		box.padding = 0.5,
		min.segment.length = 0, 
		max.overlaps = Inf,
		direction = "y") +
	# ... plus me
	geom_label_repel(
		data = filter(ff_user_count, username == "joel_nitta"),
		fill = "light blue",
		min.segment.length = 0,
		aes(label = username)) +
	scale_fill_viridis_d() +
	scale_x_continuous(expand = c(0,0)) +
	labs(x = "n tweets", fill = "Singleton") +
	theme(
		axis.text.y = element_blank(),
		axis.ticks.y = element_blank(),
		legend.position = "bottom")
```

Wow, some people are really on top of this hashtag! I recognize some of these... [\@ALULUALULU_M](https://twitter.com/ALULUALULU_M) takes really nice photos, and [\@ja_pelosi](https://twitter.com/ja_pelosi) has been posting informative threads about one fern family per week at [\#51WeeksOfPteridophytes](https://twitter.com/hashtag/51WeeksOfPteridophytes?src=hashtag_click).

Again we see a similar pattern to species: there are a small number of very regular [#fernfriday](https://twitter.com/hashtag/fernfriday?src=hashtag_click) tweeters, then a long tail of those who use the hashtag only occasionally... or just once (ahem)!

Well I might not have the most [#fernfriday](https://twitter.com/hashtag/fernfriday?src=hashtag_click) tweets, but I think I'm definitely in the running for best [#fernfriday](https://twitter.com/hashtag/fernfriday?src=hashtag_click) blogpost :)

<!--------------- appendices ----------------->

```{r, echo = FALSE}
refinery::insert_appendix(
  repo_spec = params$repo, 
  name = paste(params$date, params$slug, sep = "_"),
  source_branch = "main", lockfile_branch = "main"
)
```


<!--------------- miscellanea ----------------->

```{r redirect, echo = FALSE}
refinery::insert_netlify_redirect(
  slug = params$slug, 
  date = params$date
)
```
