---
title: "#fernfriday 2021"
description: |
  In which I answer the burning question on everybody's minds: what was
  the most popular fern of 2021??
author:
  - first_name: "Joel"
    last_name: "Nitta"
    url: https://joelnitta.com
    affiliation: University of Tokyo
    affiliation_url: http://iwasakilab.k.u-tokyo.ac.jp
    orcid_id: 0000-0003-4719-7472
date: 2022-01-24
preview: 'img/new-zealand-fantail-3729278_960_720.jpg'
creative_commons: CC BY
citation_url: https://joelnitta.com/friday-ferns-2021 
repository_url: https://github.com/joelnitta/joelnitta-home
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_depth: 3
params:
  slug: friday-ferns-2021
  date: 2022-01-24
  repo: joelnitta/joelnitta-home
  site: https://joelnitta.com
editor_options: 
  chunk_output_type: console
draft: true
---

<!--------------- setup post ----------------->

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE)
refinery::renv_load(paste(params$date, params$slug, sep = "_"))
library(tweetrmd)
library(webshot2)
```

```{r xaringanExtra-clipboard, echo = FALSE}
# Enable copy-paste of code chunks
# xaringanExtra::use_clipboard()
htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "<i class=\"fa fa-clipboard fa-2x\"></i>",
    success_text = "<i class=\"fa fa-check fa-2x\" style=\"color: #90BE6D\"></i>",
    error_text = "<i class=\"fa fa-times-circle fa-2x\" style=\"color: #F94144\"></i>"
  ),
  rmarkdown::html_dependency_font_awesome()
)
```

<!--------------- post ----------------->

## tl;dr

I show how to analyze #fernfriday tweets from 2021, including

- Scraping tweets with `snscraper`
- Extracting species names with `gntagger`
- Loading JSON data into R with `jsonlite`

```{r fern-bird}
#| echo = FALSE,
#| fig.cap = "Image by [jewelie108](https://pixabay.com/users/jewelie108-2884314/) on [pixabay](https://pixabay.com/photos/new-zealand-fantail-bird-3729278/)." 

knitr::include_graphics("img/new-zealand-fantail-3729278_960_720.jpg")
```

One of the saving graces of twitter is the existence of things like [#fernfriday](https://twitter.com/hashtag/fernfriday?src=hashtag_click) and other hashtags of people's favorite organisms.

Recently, [\@HannahOish](https://twitter.com/HannahOish) posted [this brief analysis](https://twitter.com/HannahOish/status/1477267832018776068) of the [#FishADay](https://twitter.com/hashtag/FishADay?src=hashtag_click) hashtag:

```{r hannah-tweet, echo = FALSE, cache = TRUE}
tweet_screenshot(tweet_url("HannahOish", "1477267832018776068"))
```

Upon seeing it, I immediately knew what I had to do next: **find out the most popular fern species of [#fernfriday](https://twitter.com/hashtag/fernfriday?src=hashtag_click) in 2021!**

First let's load some packages.

```{r load-packages, cache = FALSE}
library(tidyverse)
library(janitor)
library(fuzzyjoin)
```

## Getting tweets with snscrape

There are at least two R packages to get twitter data in R,  [rtweet](https://github.com/ropensci/rtweet) and [tweetR](https://cran.r-project.org/web/packages/twitteR/README.html)

There is just one problem: they rely on the [official twitter API](https://developer.twitter.com/en/docs/tutorials/getting-started-with-r-and-v2-of-the-twitter-api), which is **limited to tweets in the past week to 10 days or so**, unless you want to shell out \$\$\$ for a paid API account... which I don't.

So I will get around this pesky problem by using the [snscrape](https://github.com/JustAnotherArchivist/snscrape) twitter scraper written in python ^[This is probably not within twitter's user guidelines, so use at your own discretion].

### Installation

snscrape can be installed using [pip](https://pip.pypa.io/en/stable/).

My preferred method for handling python is to use the conda environment manager. We can [use pip within conda](https://www.anaconda.com/blog/using-pip-in-a-conda-environment) to install snscrape by writing an `environment.yml` file like this (thanks to [\@hansheng0512](https://github.com/hansheng0512/tweets-scrapping-using-python)):

```
name: tweets-scraping
dependencies:
- python=3.8
- pip=21.1.3
- pip:
- git+https://github.com/JustAnotherArchivist/snscrape.git
```

Once that is in your working directory, run this to create the environment:

```
conda env create -f environment.yml
```

Now snscrape should be installed and ready to use!

### Scraping

Since `snscrape` is a command-line interface (CLI), the normal way to use it would be to type commands from the command line. But this is an R blog, so let's run it from R using the `system()` command ^[A good way to limit the number of hits when you are testing code is with the `--max-results` option, e.g., `--max-results 10`] ^[Again, inspired by [\@hansheng0512]( https://github.com/hansheng0512/tweets-scrapping-using-python/blob/master/main.py)'s code].

```{r scrape-tweets, cache = TRUE}
# Set some variables, in case we want to modify the search later
DATE_START <- "2021-01-01"
DATE_END <- "2021-12-31"
HASHTAG <- "fernfriday"
JSON_FILENAME = "ff-2021-tweets"

# Compose the command for snscrape to retrieve tweets
command <- glue::glue('snscrape --jsonl --since {DATE_START} twitter-hashtag "{HASHTAG} until:{DATE_END}" > {JSON_FILENAME}.json')

# Run the command inside the conda env
system(
  glue::glue(
    "source ~/miniconda3/etc/profile.d/conda.sh ; 
			 conda activate tweets-scraping ; 
			 {command} ; 
			 conda deactivate")
)
```

Now the tweets have been downloaded in JSON format to `ff-2021-tweets.json`.

### Loading tweets into R

We can [load the tweets into R](https://stackoverflow.com/questions/35276184/json-using-jsonlite-parsing-error-in-r) with the `jsonlite` package.

```{r load-tweets-show, echo = TRUE, eval = FALSE}
ff_tweets_raw <- jsonlite::stream_in(file("ff-2021-tweets.json"))
```

```{r load-tweets-run, echo = FALSE, eval = TRUE}
ff_tweets_raw <- jsonlite::stream_in(file(here::here("_posts/2022-01-24_friday-ferns-2021/ff-2021-tweets.json")))
```

Let's take a peek at the data.

```{r peek-data}
glimpse(ff_tweets_raw)
```

There's a lot of stuff in there! Let's clean up the data a bit and just keep the useful bits.

```{r clean-data}
ff_tweets <-
  ff_tweets_raw %>%
    as_tibble() %>%
    select(
      date, url, user, id,
      content = renderedContent,
      lang, coordinates, place,
      contains("count")
    ) %>%
  # Extract data from nested dataframes 
  mutate(
    user = select(user, username, followersCount),
    place = select(place, country),
    coordinates = select(coordinates, longitude, latitude)
    ) %>%
  unnest(c(user, place, coordinates)) %>%
  clean_names()

ff_tweets
```

That's better.

## Extracting scientific names with gntagger

So getting our twitter data wasn't that hard, now just to find all the fern species...

Uh-oh. This is easier said than done. Twitter doesn't have a JSON field for fern species!

Luckily this post *does* have some biological content. For this task I will use [`gntagger`](https://github.com/gnames/gntagger). `gntagger` is a fantastic little CLI written in GO that handles exactly this sort of situation: **it automatically detects species names in raw text** ^[OK it was probably designed more with old biodiversity literature in mind and not twitter, but it works fine either way!].

Installation [is quite simple as described in the `gntagger` docs](https://github.com/gnames/gntagger#installation), so I won't go into that any more here.

### Tagging names

`gntagger` requires a plain-text file as input, so let's write out the tweets to a plain text file with one line per tweet:

```{r gntagger-run, echo = FALSE, eval = FALSE}
ff_tweets %>%
  # Replace line breaks with spaces so we get one tweet per line
  mutate(content = str_replace_all(content, "\n", " ")) %>%
  pull(content) %>%
  write_lines(
    here::here("_posts/2022-01-24_friday-ferns-2021/ff-2021-tweet-content.txt"))
```

```{r gntagger-show, echo = TRUE, eval = TRUE}
ff_tweets %>%
  # Replace line breaks with spaces so we get one tweet per line
  mutate(content = str_replace_all(content, "\n", " ")) %>%
  pull(content) %>%
  write_lines("ff-2021-tweet-content.txt")
```

Now, open the text file with `gntagger` ^[You may need to execute this with `./gntagger` if you didn't make `gntagger` executable everywhere by putting it on your `PATH`]:

```
gntagger ff-2021-tweet-content.txt
```

You should see a screen that looks like this:

![](img/gntagger.png)

`gntagger` takes a "android" approach to tagging species names. It first uses its own algorithm to automatically identify all possible species names, then has a very simple user interface (UI) so the user can confirm or reject each candidate name rapidly. According to the [`gntagger` docs](https://github.com/gnames/gntagger#gntagger-), you should be able to get through about "4000 names spread over 600 pages in about 2 hours".

It saves the intermediate output to a folder named after the input file, in this case `ff-2021-tweet-content.txt_gntagger`.

### Reading gntagger output back into R

I recommend trying out `gntagger` to see how it works---it definitely looks great for parsing names from old literature!

But you don't have to go through all of the names yourself. In fact, `gntagger`'s initial "guesses" are pretty darn good, and can be used as-is for a rough analysis. Or, you can download the cleaned up version from FIXME: ADD LINK.

Let's read in the `gntagger` results, which are again JSON:

```{r load-gntagger-show, echo = TRUE, eval = FALSE}
ff_species_raw <- jsonlite::fromJSON("ff-2021-tweet-content.txt_gntagger/names.json")

ff_species <- ff_species_raw[["names"]] %>% 
  as_tibble() %>%
		# I'll call the name output by gntagger "taxon", since
	# not all of them are species
  select(type, taxon = name, start, end)

ff_species
```

```{r load-gntagger-run, echo = FALSE, eval = TRUE}
ff_species_raw <- jsonlite::fromJSON(here::here("_posts/2022-01-24_friday-ferns-2021/ff-2021-tweet-content.txt_gntagger/names.json"))

ff_species <- ff_species_raw[["names"]] %>% 
  as_tibble() %>%
	# I'll call the name output by gntagger "taxon", since
	# not all of them are species
  select(type, taxon = name, start, end)

ff_species
```

The tricky part here is that `gntagger` outputs location by character position, not line.

So we need to figure out which characters match which lines, which in turn match tweets...

```{r calc_start_end}
ff_tweets_start_end <-
  ff_tweets %>%
  mutate(
    # Replace line breaks with spaces so we get one tweet per line
    content = str_replace_all(content, "\n", " "),
    # Count number of characters per tweet
    num_char = nchar(content)) %>%
  select(id, num_char) %>%
  # Calculate start and end of each tweet in characters
  mutate(
    end = cumsum(num_char),
    start = end - num_char + 1,
    start = as.integer(start)) %>%
  select(id, start, end)

ff_tweets_start_end
```

Now we can use the `fuzzyjoin` package to join the species names to the tweets by position.

```{r fuzzy-join}
ff_tweets_species <-
ff_tweets %>%
  left_join(ff_tweets_start_end, by = "id") %>%
  # interval_left_join requires the Bioconductor IRanges package to be installed
	# and joins by columns "start" and "end" by default
  interval_left_join(
    ff_species
  ) %>%
  select(-matches("start|end")) %>%
  # Drop tweets without any fern names mentioned
  filter(!is.na(taxon))
```

Let's take a peek at the results.

```{r peek-post-join}
select(ff_tweets_species, content, taxon)
```

Yay! We've got species names mapped to tweets (and user ID, etc). Notice that some tweets now appear duplicated, since our data are have one row per species mentioned, and some tweets may include multiple species.

## Data analysis (finally!)

OK, we can finally analyze the data and see **who is the most popular fern of 2021**!

I guess a simple approach would be to see what fern got the most tweets?

```{r top-species}
ff_tweets_species %>%
	# Filter to only taxa that look like species names
	filter(str_detect(type, "Binomial")) %>%
	# Just look at the top 10
	mutate(taxon = fct_lump_n(taxon, n = 10)) %>%
	count(taxon) %>%
	mutate(taxon = fct_reorder(taxon, n)) %>%
	ggplot(aes(x = n, y = taxon)) + geom_col()
```

That's... not that interesting. Probably because very few species are actually tweeted multiple times. Also, it's possible that some users are tweeting the same species multiple times, which is kind of unfair right?

Let's just look at the number of users tweeting about each species, and dump the "Other" category.

```{r top-species-users}
ff_tweets_species %>% 
	# Filter to only taxa that look like species names
  filter(str_detect(type, "Binomial")) %>%
	# Only allow one "vote" per species per user
	select(taxon, username) %>% 
	unique() %>% 
	# Just look at the top 10
	mutate(taxon = fct_lump_n(taxon, n = 10)) %>%
	# Exclude "Other"
	filter(taxon != "Other") %>%
	count(taxon) %>%
	mutate(taxon = fct_reorder(taxon, n)) %>%
	ggplot(aes(x = n, y = taxon)) + geom_col()

```

Cool! Those are some solid picks. And the top 10 species here all include > 3 three unique users tweeting about them.

<!--------------- appendices ----------------->

```{r, echo = FALSE}
refinery::insert_appendix(
  repo_spec = params$repo, 
  name = paste(params$date, params$slug, sep = "_"),
  source_branch = "main", lockfile_branch = "main"
)
```


<!--------------- miscellanea ----------------->

```{r redirect, echo = FALSE}
refinery::insert_netlify_redirect(
  slug = params$slug, 
  date = params$date
)
```
