---
title: "Managing bioinformatics pipelines with R"
description:
  How to run a complete bioinformatics pipeline from the comfort of R
author:
  - first_name: "Joel"
    last_name: "Nitta"
    url: https://joelnitta.com
    affiliation: University of Tokyo
    affiliation_url: http://iwasakilab.k.u-tokyo.ac.jp
    orcid_id: 0000-0003-4719-7472
date: 2021-11-16
preview: "img/t-k-9AxFJaNySB8-unsplash.jpg"
creative_commons: CC BY
citation_url: https://joelnitta.com/r-bioinfo-flow 
repository_url: https://github.com/joelnitta/joelnitta-home
output:
  distill::distill_article:
    self_contained: false
params:
  slug: r-bioinfo-flow
  date: 2021-11-16
  repo: joelnitta/joelnitta-home
  site: https://joelnitta.com
draft: false
editor_options: 
  chunk_output_type: console
---

<!-- IMPORTANT: set wd to "_posts/2021-11-16_r_bioinfo_flow" before knitting 
so targets can find the cache -->

<!--------------- setup post ----------------->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
refinery::renv_load(paste(params$date, params$slug, sep = "_"))
```

<!--------------- post ----------------->

## tl;dr

- `targets` is great for managing bioinformatics workflows
- `renv`, Conda, and Docker can be combined so that all steps are modular and reproducible
- Demo available at https://github.com/joelnitta/targets_bioinfo_example

```{r image-1}
#| echo = FALSE,
#| fig.cap = "Image by [T K](https://unsplash.com/@realaxer) on [unsplash](https://unsplash.com/photos/9AxFJaNySB8)." 

knitr::include_graphics("img/t-k-9AxFJaNySB8-unsplash.jpg")
```

Bioinformatics projects tend to have a similar pattern: they all start with raw data, then pass the data through various programs until arriving at the final result. These "pipelines" can become very long and complicated, so there are many platforms that automate this process either relying on code (e.g., [nextflow](https://www.nextflow.io/), [CWL](https://www.commonwl.org/)) or graphical interfaces (e.g., [galaxy](https://galaxyproject.org/)). Python's [snakemake](https://snakemake.readthedocs.io/en/stable/) is also commonly used for this purpose. That got me thinking - can we do this in R?

## What I'm looking for in a pipeline manager

These are some qualities that I want to see in a pipeline manager.

1. **Automated**: I should be able to run one central script that will orchestrate the whole pipeline, rather than manually keeping track of which step depends on which and when each needs to be run. 
2. **Efficient**: The pipeline manager should keep track of what is out of date and only re-run those parts, rather than re-run the whole pipeline each time.
3. **Reproducible**: Software packages should be isolated and version controlled so that the same input results in the same output on any machine.

## Enter `targets`

The [targets](https://docs.ropensci.org/targets/) R package pretty much fits the bill perfectly for Points **1** and **2**. `targets` completely automates the workflow, so that the user doesn't have to manually run steps, and guarantees that the output is up-to-date (if the workflow is designed correctly). Furthermore, it has capabilities for easily looping and running processes in parallel, so it scales quite well to large analyses. I won't go into too many details of how to use `targets` here, since it has an excellent [user manual](https://books.ropensci.org/targets/).

## `targets` meets Docker

However, `targets` by itself isn't quite enough to meet all of my bioinformatics needs. What about Point **3**--how can we make `targets` workflows reproducible?

Most bioinformatics tools are open-source software packages that have a command-line interface (CLI). Furthermore, these days, most well-established bioinformatics tools have Docker images[^1] available to run them, such as on [biocontainers](https://biocontainers.pro/). These free us from manual installations and [dependency hell](https://en.wikipedia.org/wiki/Dependency_hell), as well as vastly improving reproducibility, since all the software versions are fixed within the container. So **I will run most of the steps of the pipeline in available Docker containers**.

## Avoiding Docker-in-Docker

```{r image-2}
#| echo = FALSE,
#| fig.cap = "Image by [Giordano Rossoni](https://unsplash.com/@reddgio) on [unsplash](https://unsplash.com/photos/czu8X_gfpP0)." 

knitr::include_graphics("img/giordano-rossoni-czu8X_gfpP0-unsplash.jpg")
```

However, I then encounter a problem: what about the environment to run R, `targets`, and launch the Docker containers? That environment should be version-controlled and reproducible too. Normally my solution to create such an environment **is** Docker, but it's generally a bad idea to try and run [docker from within docker](https://jpetazzo.github.io/2015/09/03/do-not-use-docker-in-docker-for-ci/)[^2]

The solution I reached is to use two more environment managers: [Conda](https://docs.conda.io/en/latest/)[^3] and [renv](https://rstudio.github.io/renv/articles/renv.html). I use Conda for **running R**, and `renv` for **managing R packages**.

## Running R with Conda

Conda environments [can be recorded using a `yaml` file](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#creating-an-environment-from-an-environment-yml-file). This is my Conda `environment.yaml` file:

```
name: renv
channels:
  - conda-forge
  - bioconda
  - defaults
dependencies:
  - r-renv=0.14.*
```

It's quite short: all it does is install `renv` and its dependencies (which includes R). Here I've specified the most recent major version[^4] of `renv`, which will come with R v4.1.1.

We can recreate the conda environment from the `yaml` file (after [installing conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html)) with:

<!-- Chunk to show, but don't run-->
```{bash, create-env-show, eval = FALSE, echo = TRUE}
conda env create -f environment.yml
```

```{bash, create-env-hide, eval = TRUE, echo = FALSE, include = TRUE}
# Chunk to actually run, but don't show
# need to manually source conda when knitting
source ~/miniconda3/etc/profile.d/conda.sh
# remove any existing renv environment
# conda remove --name renv --all --yes
# build env
conda env create -f environment.yml
```

As the output says near the bottom, run `conda activate renv` to enter this environment, then from there you can use R as usual. Now we have a fixed version of R, with a fixed version of `r-env`.

## Maintain R packages with `renv`

The next step is to use `renv` to install and track R package versions. `renv` does this with a "lock file", which is essentially a specification of every package needed to run the code, its version, and where it comes from. This is what a typical lock file looks like (in part):

```
{
  "R": {
    "Version": "4.1.1",
    "Repositories": [
      {
        "Name": "CRAN",
        "URL": "https://cloud.r-project.org"
      }
    ]
  },
  "Packages": {
    "Matrix": {
      "Package": "Matrix",
      "Version": "1.3-4",
      "Source": "Repository",
      "Repository": "CRAN",
      "Hash": "4ed05e9c9726267e4a5872e09c04587c"
    },
...
```

I've provided an `renv.lock` file that includes all the packages needed to run the workflow [here](https://raw.githubusercontent.com/joelnitta/targets_bioinfo_example/main/renv.lock).

To install the R packages, [download](https://raw.githubusercontent.com/joelnitta/targets_bioinfo_example/main/renv.lock) the `renv.lock` file and put in the current working directory. Then from R within the conda environment, run `renv::restore()`:

```
renv::restore()
```

You should see something like this[^5]:

```
(renv) Joels-iMac:targets_bioinfo_example joelnitta$ Rscript -e 'renv::restore()'
The following package(s) will be updated:

# CRAN ===============================
- Matrix          [* -> 1.3-4]
- R6              [* -> 2.5.1]
- Rcpp            [* -> 1.0.7]
- RcppArmadillo   [* -> 0.10.6.0.0]
- RcppParallel    [* -> 5.1.4]
- assertthat      [* -> 0.2.1]
- babelwhale      [* -> 1.0.3]
- callr           [* -> 3.7.0]

...
```

## Putting it all together

OK, now we have all of the pieces in place, and can run Docker from a reproducible environment. What is the best way to run Docker from R? There are some functions in base R for running external commands (`system()`, `system2()`) as well as the excellent [processx](https://github.com/r-lib/processx) package. Here, though I will use the [babelwhale](https://github.com/dynverse/babelwhale) package, which provides some nice wrappers to run Docker (or [Singularity](https://sylabs.io/singularity/)).

Here is an example `_targets.R` file using `babelwhale` to run Docker. This workflow downloads a pair of fasta files, then trims low-quality bases using the [fastp](https://github.com/OpenGene/fastp) program [^6].

```{r example-targets, eval = FALSE, include = TRUE}
library(targets)
library(tarchetypes)
library(babelwhale)

# Set babelwhale backend for running containers
# (here, we are using Docker, not Singularity)
set_default_config(create_docker_config())

# Define workflow
list(
	# Download example fastq files
	tar_file(
		read_1, { 
			download.file(
				url = "https://raw.githubusercontent.com/OpenGene/fastp/master/testdata/R1.fq",
				destfile = "R1.fq")
			"R1.fq"
		}
	),
	tar_file(
		read_2, { 
			download.file(
				url = "https://raw.githubusercontent.com/OpenGene/fastp/master/testdata/R2.fq",
				destfile = "R2.fq")
			"R2.fq"
		}
	),
	# Clean the fastq file with fastp
	tar_file(
		cleaned_fasta_file, {
			babelwhale::run(
				# Name of docker image, with tag specifying version
				"quay.io/biocontainers/fastp:0.23.1--h79da9fb_0",
				# Command to run
				command = "fastp",
				# Arguments to the command
				args = c(
					# fastq input files
					"-i", paste0("/wd/", read_1), 
					"-I", paste0("/wd/", read_2), 
					# fastq output files
					"-o", "/wd/R1_trim.fq",
				  "-O", "/wd/R2_trim.fq",
					# trim report file
					"-h", "/wd/trim_report.html"),
				# Volume mounting specification
				# this uses getwd(), but here::here() is also a good method
				volumes = paste0(getwd(), ":/wd/")
			)
			c("R1_trim.fq", "R2_trim.fq", "trim_report.html")
		}
	)
)
```

```{r clean-targets, eval = TRUE, include = FALSE, echo = FALSE}
# Clean the targets cache before running anything
targets::tar_destroy("all")
```

In order to run this `targets` workflow, the above code should be saved as `_targets.R` in the project root directory. Then, run `targets::tar_make()`, sit back, and enjoy the show:

```{r build-targets}
targets::tar_make()
```

You should be able to confirm that the read files were downloaded, cleaned, and a report generated in your working directory.

## Next steps

```{r image-3}
#| echo = FALSE,
#| fig.cap = "Image by [JOHN TOWNER](https://unsplash.com/@heytowner) on [unsplash](https://unsplash.com/photos/3Kv48NS4WUU)." 

knitr::include_graphics("img/john-towner-3Kv48NS4WUU-unsplash.jpg")
```

The example workflow just consists of a couple of steps, but I hope you can see how they are chained together: `cleaned_fasta_file` depends on `read_1` and `read_2`. We could add a third step that uses `cleaned_fasta_file` for something else, and so forth. `targets` will keep track of the order that steps need to be run, and only re-run outdated steps if any parts of the workflow change and we run `targets::tar_make()` again.

To keep things simple for this post, I have written the workflow as a single R script, but that's not really the ideal way to do it. You can see that the syntax is rather verbose, and such a script would rapidly become very long. The best practice for `targets` workflows is to write the targets plan and the functions that build each target separately, as `_targets.R` and `functions.R`, respectively.

By splitting the plan from the functions this way, our `_targets.R` file becomes much shorter and more readable:

```{r real-plan, eval = FALSE, include = TRUE}
library(targets)
library(tarchetypes)
library(babelwhale)

# Set babelwhale backend for running containers
set_default_config(create_docker_config())

# Load functions
source("R/functions.R")

tar_plan(
	# Download example fastq files
	tar_file(read_1, download_read("R1.fq")),
	tar_file(read_2, download_read("R2.fq")),
	# Clean the fastq files with fastp
	tar_file(
		fastp_out, 
		fastp(read_1, read_2, "R1_trim.fq", "R2_trim.fq", "trim_report.html"
		)
	)
)
```

You can see how it provides a high-level overview of each step in the workflow, without getting bogged down in the details. And the best part is, you **don't have to install `fastp`** (or any other software used for a particular step)! Docker takes care of that for you. 

I have made this plan and the accompanying `functions.R` file available at this repo: https://github.com/joelnitta/targets_bioinfo_example. Please check it out!

## Conclusion

I am really excited about using `targets` for reproducibly managing bioinformatics workflows from R. I hope this helps others who may want to do the same!

[^1]: [Docker images]() are basically completely self-contained computing environments, such that the software inside the image is exactly the same no matter where it is run. A major benefit of using a docker image is that you don't have to install all of the various dependencies for a particular package: it all comes bundled in the image. And if the image has been tagged (versioned) correctly, you can specify the exact software version and know that the results won't change in the future.

[^2]: Think *Inception*.

[^3]: Conda was originally developed for managing python and python packages, but it has expanded greatly and works as a general software package manager.

[^4]: The asterisk in `r-renv=0.14.*` indicates to install the most recent version with the `0.14` version number.

[^5]: I'm not actually running this command and showing the output, since this post is already rendered using `renv`, and running `renv` within `renv` is also getting too *Inception*-y!

[^6]: I won't go into the details of the `targets` syntax here, but I highly recommend [this chapter in the `targets` manual](https://books.ropensci.org/targets/files.html) for working with external files, which are very common in bioinformatics workflows.

<!--------------- appendices ----------------->

```{r, echo=FALSE}
refinery::insert_appendix(
  repo_spec = params$repo, 
  name = paste(params$date, params$slug, sep = "_")
)
```
